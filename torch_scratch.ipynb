{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args = Namespace(N_importance=128, N_rand=1024, N_samples=64, basedir='./logs', chunk=32768, config='configs/lego.txt', datadir='./data/nerf_synthetic/lego', dataset_type='blender', expname='blender_paper_lego', factor=8, ft_path=None, half_res=True, i_embed=0, i_img=500, i_print=100, i_testset=50000, i_video=50000, i_weights=10000, lindisp=False, llffhold=8, lrate=0.0005, lrate_decay=500, multires=10, multires_views=4, netchunk=65536, netdepth=8, netdepth_fine=8, netwidth=256, netwidth_fine=256, no_batching=True, no_ndc=False, no_reload=False, perturb=1.0, precrop_frac=0.5, precrop_iters=500, raw_noise_std=0.0, render_factor=0, render_only=False, render_test=False, shape='greek', spherify=False, testskip=8, use_viewdirs=True, white_bkgd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imageio\n",
    "import json\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "trans_t = lambda t : torch.Tensor([\n",
    "    [1,0,0,0],\n",
    "    [0,1,0,0],\n",
    "    [0,0,1,t],\n",
    "    [0,0,0,1]]).float()\n",
    "\n",
    "rot_phi = lambda phi : torch.Tensor([\n",
    "    [1,0,0,0],\n",
    "    [0,np.cos(phi),-np.sin(phi),0],\n",
    "    [0,np.sin(phi), np.cos(phi),0],\n",
    "    [0,0,0,1]]).float()\n",
    "\n",
    "rot_theta = lambda th : torch.Tensor([\n",
    "    [np.cos(th),0,-np.sin(th),0],\n",
    "    [0,1,0,0],\n",
    "    [np.sin(th),0, np.cos(th),0],\n",
    "    [0,0,0,1]]).float()\n",
    "\n",
    "\n",
    "def pose_spherical(theta, phi, radius):\n",
    "    c2w = trans_t(radius)\n",
    "    c2w = rot_phi(phi/180.*np.pi) @ c2w\n",
    "    c2w = rot_theta(theta/180.*np.pi) @ c2w\n",
    "    c2w = torch.Tensor(np.array([[-1,0,0,0],[0,0,1,0],[0,1,0,0],[0,0,0,1]])) @ c2w\n",
    "    return c2w\n",
    "\n",
    "\n",
    "def load_blender_data(basedir, half_res=False, testskip=1):\n",
    "    splits = ['train', 'val', 'test']\n",
    "    metas = {}\n",
    "    for s in splits:\n",
    "        with open(os.path.join(basedir, 'transforms_{}.json'.format(s)), 'r') as fp:\n",
    "            metas[s] = json.load(fp)\n",
    "\n",
    "    all_imgs = []\n",
    "    all_poses = []\n",
    "    counts = [0]\n",
    "    for s in splits:\n",
    "        meta = metas[s]\n",
    "        imgs = []\n",
    "        poses = []\n",
    "        if s=='train' or testskip==0:\n",
    "            skip = 1\n",
    "        else:\n",
    "            skip = testskip\n",
    "            \n",
    "        for frame in meta['frames'][::skip]:\n",
    "            fname = os.path.join(basedir, frame['file_path'] + '.png')\n",
    "            imgs.append(imageio.imread(fname))\n",
    "            poses.append(np.array(frame['transform_matrix']))\n",
    "        imgs = (np.array(imgs) / 255.).astype(np.float32) # keep all 4 channels (RGBA)\n",
    "        poses = np.array(poses).astype(np.float32)\n",
    "        counts.append(counts[-1] + imgs.shape[0])\n",
    "        all_imgs.append(imgs)\n",
    "        all_poses.append(poses)\n",
    "    \n",
    "    i_split = [np.arange(counts[i], counts[i+1]) for i in range(3)]\n",
    "    \n",
    "    imgs = np.concatenate(all_imgs, 0)\n",
    "    poses = np.concatenate(all_poses, 0)\n",
    "    \n",
    "    H, W = imgs[0].shape[:2]\n",
    "    camera_angle_x = float(meta['camera_angle_x'])\n",
    "    focal = .5 * W / np.tan(.5 * camera_angle_x)\n",
    "    \n",
    "    render_poses = torch.stack([pose_spherical(angle, -30.0, 4.0) for angle in np.linspace(-180,180,40+1)[:-1]], 0)\n",
    "    \n",
    "    if half_res:\n",
    "        H = H//2\n",
    "        W = W//2\n",
    "        focal = focal/2.\n",
    "\n",
    "        imgs_half_res = np.zeros((imgs.shape[0], H, W, 4))\n",
    "        for i, img in enumerate(imgs):\n",
    "            imgs_half_res[i] = cv2.resize(img, (W, H), interpolation=cv2.INTER_AREA)\n",
    "        imgs = imgs_half_res\n",
    "        # imgs = tf.image.resize_area(imgs, [400, 400]).numpy()\n",
    "\n",
    "        \n",
    "    return imgs, poses, render_poses, [H, W, focal], i_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0_/jlz3w3sd0rvgpp49r0h93j_c0000gn/T/ipykernel_82092/2375020381.py:56: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  imgs.append(imageio.imread(fname))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded blender (138, 400, 400, 4) torch.Size([40, 4, 4]) [400, 400, 555.5555155968841] ./data/nerf_synthetic/lego\n"
     ]
    }
   ],
   "source": [
    "# For --config configs/lego.txt\n",
    "\n",
    "datadir = \"./data/nerf_synthetic/lego\"\n",
    "half_res = True\n",
    "testskip = 8\n",
    "images, poses, render_poses, hwf, i_split = load_blender_data(datadir,half_res,testskip)\n",
    "\n",
    "print('Loaded blender', images.shape, render_poses.shape, hwf,datadir)\n",
    "i_train, i_val, i_test = i_split\n",
    "\n",
    "near = 2.\n",
    "far = 6.\n",
    "images= images[...,:3]*images[...,-1:] + (1.-images[...,-1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W, focal = hwf\n",
    "H, W = int(H), int(W)\n",
    "hwf = [H, W, focal]\n",
    "K = None\n",
    "if K is None:\n",
    "    K = np.array([\n",
    "        [focal, 0, 0.5*W],\n",
    "        [0, focal, 0.5*H],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "\n",
    "if args.render_test:\n",
    "    render_poses = np.array(poses[i_test])\n",
    "\n",
    "# Create log dir and copy the config file\n",
    "basedir = basedir\n",
    "expname = args.expname\n",
    "os.makedirs(os.path.join(basedir, expname), exist_ok=True)\n",
    "f = os.path.join(basedir, expname, 'args.txt')\n",
    "with open(f, 'w') as file:\n",
    "    for arg in sorted(vars(args)):\n",
    "        attr = getattr(args, arg)\n",
    "        file.write('{} = {}\\n'.format(arg, attr))\n",
    "if args.config is not None:\n",
    "    f = os.path.join(basedir, expname, 'config.txt')\n",
    "    with open(f, 'w') as file:\n",
    "        file.write(open(args.config, 'r').read())\n",
    "\n",
    "# Create nerf model\n",
    "render_kwargs_train, render_kwargs_test, start, grad_vars, optimizer = create_nerf(args)\n",
    "global_step = start"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
